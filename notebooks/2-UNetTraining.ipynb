{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/adadelta.py:74: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adadelta, self).__init__(name, **kwargs)\n",
      "/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/nadam.py:73: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Nadam, self).__init__(name, **kwargs)\n",
      "/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/adagrad.py:74: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adagrad, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "\n",
    "import os\n",
    "import time\n",
    "import rasterio.warp  # Reproject raster samples\n",
    "from functools import reduce\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from core.UNet import UNet\n",
    "from core.losses import (\n",
    "    tversky,\n",
    "    accuracy,\n",
    "    dice_coef,\n",
    "    dice_loss,\n",
    "    specificity,\n",
    "    sensitivity,\n",
    ")\n",
    "from core.optimizers import adaDelta\n",
    "from core.frame_info import FrameInfo\n",
    "from core.dataset_generator import DataGenerator\n",
    "from core.split_frames import split_dataset\n",
    "from core.visualize import display_images\n",
    "\n",
    "import warnings  # ignore annoying warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required configurations (including the input and output paths) are stored in a separate file (such as config/UNetTraining.py)\n",
    "# Please provide required info in the file before continuing with this notebook.\n",
    "\n",
    "from config import UNetTraining\n",
    "\n",
    "# In case you are using a different folder name such as configLargeCluster, then you should import from the respective folder\n",
    "# Eg. from configLargeCluster import UNetTraining\n",
    "config = UNetTraining.Configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all images/frames into memory\n",
    "frames = []\n",
    "\n",
    "all_files = os.listdir(config.base_dir)\n",
    "all_files_ndvi = [\n",
    "    fn\n",
    "    for fn in all_files\n",
    "    if fn.startswith(config.ndvi_fn) and fn.endswith(config.image_type)\n",
    "]\n",
    "len(all_files_ndvi)\n",
    "for i, fn in enumerate(all_files_ndvi):\n",
    "    ndvi_img = rasterio.open(os.path.join(config.base_dir, fn))\n",
    "    pan_img = rasterio.open(\n",
    "        os.path.join(config.base_dir, fn.replace(config.ndvi_fn, config.pan_fn))\n",
    "    )\n",
    "    read_ndvi_img = ndvi_img.read()\n",
    "    read_pan_img = pan_img.read()\n",
    "    comb_img = np.concatenate((read_ndvi_img, read_pan_img), axis=0)\n",
    "    comb_img = np.transpose(comb_img, axes=(1, 2, 0))  # Channel at the end\n",
    "    annotation_im = Image.open(\n",
    "        os.path.join(config.base_dir, fn.replace(config.ndvi_fn, config.annotation_fn))\n",
    "    )\n",
    "    annotation = np.array(annotation_im)\n",
    "    weight_im = Image.open(\n",
    "        os.path.join(config.base_dir, fn.replace(config.ndvi_fn, config.weight_fn))\n",
    "    )\n",
    "    weight = np.array(weight_im)\n",
    "    f = FrameInfo(comb_img, annotation, weight)\n",
    "    frames.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_frames, validation_frames, testing_frames = split_dataset(\n",
    "    frames, config.frames_json, config.patch_dir\n",
    ")\n",
    "# training_frames = validation_frames = testing_frames  = list(range(len(frames)))\n",
    "\n",
    "annotation_channels = config.input_label_channel + config.input_weight_channel\n",
    "train_generator = DataGenerator(\n",
    "    config.input_image_channel,\n",
    "    config.patch_size,\n",
    "    training_frames,\n",
    "    frames,\n",
    "    annotation_channels,\n",
    "    augmenter=\"iaa\",\n",
    ").random_generator(config.BATCH_SIZE, normalize=config.normalize)\n",
    "val_generator = DataGenerator(\n",
    "    config.input_image_channel,\n",
    "    config.patch_size,\n",
    "    validation_frames,\n",
    "    frames,\n",
    "    annotation_channels,\n",
    "    augmenter=None,\n",
    ").random_generator(config.BATCH_SIZE, normalize=config.normalize)\n",
    "test_generator = DataGenerator(\n",
    "    config.input_image_channel,\n",
    "    config.patch_size,\n",
    "    testing_frames,\n",
    "    frames,\n",
    "    annotation_channels,\n",
    "    augmenter=None,\n",
    ").random_generator(config.BATCH_SIZE, normalize=config.normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1):\n",
    "    train_images, real_label = next(train_generator)\n",
    "    ann = real_label[:, :, :, 0]\n",
    "    wei = real_label[:, :, :, 1]\n",
    "    # overlay of annotation with boundary to check the accuracy\n",
    "    # 5 images in each row are: pan, ndvi, annotation, weight(boundary), overlay of annotation with weight\n",
    "    overlay = ann + wei\n",
    "    overlay = overlay[:, :, :, np.newaxis]\n",
    "    display_images(np.concatenate((train_images, real_label, overlay), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER = adaDelta\n",
    "LOSS = tversky\n",
    "\n",
    "# Only for the name of the model in the very end\n",
    "OPTIMIZER_NAME = \"AdaDelta\"\n",
    "LOSS_NAME = \"weightmap_tversky\"\n",
    "\n",
    "# Declare the path to the final model\n",
    "# If you want to retrain an exising model then change the cell where model is declared.\n",
    "# This path is for storing a model after training.\n",
    "\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M\")\n",
    "chf = config.input_image_channel + config.input_label_channel\n",
    "chs = reduce(lambda a, b: a + str(b), chf, \"\")\n",
    "\n",
    "\n",
    "if not os.path.exists(config.model_path):\n",
    "    os.makedirs(config.model_path)\n",
    "model_path = os.path.join(\n",
    "    config.model_path,\n",
    "    \"trees_{}_{}_{}_{}_{}.h5\".format(\n",
    "        timestr, OPTIMIZER_NAME, LOSS_NAME, chs, config.input_shape[0]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# The weights without the model architecture can also be saved. Just saving the weights is more efficent.\n",
    "\n",
    "# weight_path=\"./saved_weights/UNet/{}/\".format(timestr)\n",
    "# if not os.path.exists(weight_path):\n",
    "#     os.makedirs(weight_path)\n",
    "# weight_path=weight_path + \"{}_weights.best.hdf5\".format('UNet_model')\n",
    "# print(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model and compile it\n",
    "model = UNet([config.BATCH_SIZE, *config.input_shape], config.input_label_channel)\n",
    "model.compile(\n",
    "    optimizer=OPTIMIZER,\n",
    "    loss=LOSS,\n",
    "    metrics=[dice_coef, dice_loss, specificity, sensitivity, accuracy],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for the early stopping of training, LearningRateScheduler and model checkpointing\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    LearningRateScheduler,\n",
    "    EarlyStopping,\n",
    "    ReduceLROnPlateau,\n",
    "    TensorBoard,\n",
    ")\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    model_path,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode=\"min\",\n",
    "    save_weights_only=False,\n",
    ")\n",
    "\n",
    "# reduceonplatea; It can be useful when using adam as optimizer\n",
    "# Reduce learning rate when a metric has stopped improving (after some patience,reduce by a factor of 0.33, new_lr = lr * factor).\n",
    "# cooldown: number of epochs to wait before resuming normal operation after lr has been reduced.\n",
    "reduceLROnPlat = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.33,\n",
    "    patience=4,\n",
    "    verbose=1,\n",
    "    mode=\"min\",\n",
    "    min_delta=0.0001,\n",
    "    cooldown=4,\n",
    "    min_lr=1e-16,\n",
    ")\n",
    "\n",
    "# early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=2, patience=15)\n",
    "\n",
    "log_dir = os.path.join(\n",
    "    \"./logs\",\n",
    "    \"UNet_{}_{}_{}_{}_{}\".format(\n",
    "        timestr, OPTIMIZER_NAME, LOSS_NAME, chs, config.input_shape[0]\n",
    "    ),\n",
    ")\n",
    "tensorboard = TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=0,\n",
    "    write_graph=True,\n",
    "    write_grads=False,\n",
    "    write_images=False,\n",
    "    embeddings_freq=0,\n",
    "    embeddings_layer_names=None,\n",
    "    embeddings_metadata=None,\n",
    "    embeddings_data=None,\n",
    "    update_freq=\"epoch\",\n",
    ")\n",
    "\n",
    "callbacks_list = [\n",
    "    checkpoint,\n",
    "    tensorboard,\n",
    "]  # reduceLROnPlat is not required with adaDelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = [\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=config.MAX_TRAIN_STEPS,\n",
    "        epochs=config.NB_EPOCHS,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=config.VALID_IMG_COUNT,\n",
    "        callbacks=callbacks_list,\n",
    "        workers=1,\n",
    "        #                         use_multiprocessing=True # the generator is not very thread safe\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model after training\n",
    "# If you load a model with different python version, than you may run into a problem: https://github.com/keras-team/keras/issues/9595#issue-303471777\n",
    "\n",
    "model = load_model(\n",
    "    model_path,\n",
    "    custom_objects={\n",
    "        \"tversky\": LOSS,\n",
    "        \"dice_coef\": dice_coef,\n",
    "        \"dice_loss\": dice_loss,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"specificity\": specificity,\n",
    "        \"sensitivity\": sensitivity,\n",
    "    },\n",
    "    compile=False,\n",
    ")\n",
    "\n",
    "# In case you want to use multiple GPU you can uncomment the following lines.\n",
    "# from tensorflow.python.keras.utils import multi_gpu_model\n",
    "# model = multi_gpu_model(model, gpus=2, cpu_merge=False)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=OPTIMIZER,\n",
    "    loss=LOSS,\n",
    "    metrics=[dice_coef, dice_loss, accuracy, specificity, sensitivity],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print one batch on the training/test data!\n",
    "for i in range(1):\n",
    "    test_images, real_label = next(test_generator)\n",
    "    # 5 images per row: pan, ndvi, label, weight, prediction\n",
    "    prediction = model.predict(test_images, steps=1)\n",
    "    prediction[prediction > 0.5] = 1\n",
    "    prediction[prediction <= 0.5] = 0\n",
    "    display_images(np.concatenate((test_images, real_label, prediction), axis=-1))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOeYCBzQRMr8FXNUC8za+ng",
   "collapsed_sections": [],
   "name": "step3-Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
