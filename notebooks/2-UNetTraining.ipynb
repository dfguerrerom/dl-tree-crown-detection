{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import time\n",
    "import rasterio as rio\n",
    "from functools import reduce\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from core.UNet import UNet\n",
    "from core.losses import (\n",
    "    tversky,\n",
    "    accuracy,\n",
    "    dice_coef,\n",
    "    dice_loss,\n",
    "    specificity,\n",
    "    sensitivity,\n",
    ")\n",
    "from core.optimizers import adaDelta\n",
    "\n",
    "from core.dataset_generator import DataGenerator\n",
    "from core.split_frames import split_dataset\n",
    "from core.visualize import display_images\n",
    "\n",
    "import warnings  # ignore annoying warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import core.split_frames as split\n",
    "import core.frame_info as fram\n",
    "import core.dataset_generator as dg\n",
    "import config.conf as conf\n",
    "\n",
    "conf = reload(conf)\n",
    "config = conf.Configuration()\n",
    "fram = reload(fram)\n",
    "split = reload(split)\n",
    "dg = reload(dg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all images/frames into memory\n",
    "frames = []\n",
    "norm_images = list(config.image_dir.glob(f\"*{config.image_type}\"))\n",
    "dims={}\n",
    "for i, image_path in enumerate(norm_images):\n",
    "    \n",
    "    norm_img = rio.open(image_path)\n",
    "    \n",
    "    dims[image_path.name] = {\n",
    "        \"width\": norm_img.profile[\"width\"],\n",
    "        \"height\": norm_img.profile[\"height\"],\n",
    "    }\n",
    "    \n",
    "    annotation_path = config.annotation_dir/image_path.name\n",
    "    \n",
    "    # Check if the input annotation has detected trees\n",
    "    with open(annotation_path.with_suffix(\".json\")) as f:\n",
    "        trees = len(json.load(f)[\"Trees\"])\n",
    "        \n",
    "    if trees:\n",
    "        annotation_img = Image.open(annotation_path)\n",
    "        weight_img = Image.open(config.boundary_dir/image_path.name)\n",
    "        norm_array = norm_img.read()\n",
    "        \n",
    "        # Change the order of the bands. Let the channel at the end\n",
    "        norm_array = np.transpose(norm_array, axes=(1,2,0))\n",
    "        annotation_array = np.array(annotation_img)\n",
    "        weight_array = np.array(weight_img)\n",
    "\n",
    "        frames.append(\n",
    "            fram.FrameInfo(\n",
    "                norm_array, \n",
    "                annotation_array, \n",
    "                weight_array, \n",
    "                image_path.name,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_frames, validation_frames, testing_frames = split.split_dataset(\n",
    "    frames, config.frames_json, config.patch_dir\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames[0].img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get frames id\n",
    "# [frame.id for frame in frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually select which are the frames I'd like to test\n",
    "training_frames = [frames.index(f) for f in frames[-2:]]\n",
    "validation_frames = [frames.index(frames[1])]\n",
    "testing_frames  = [frames.index(frames[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_channels = config.input_label_channel + config.input_weight_channel\n",
    "train_generator = dg.DataGenerator(\n",
    "    config.input_image_channel,\n",
    "    config.patch_size,\n",
    "    training_frames,\n",
    "    frames,\n",
    "    annotation_channels,\n",
    "    augmenter=\"iaa\",\n",
    ").random_generator(config.BATCH_SIZE, normalize=config.normalize)\n",
    "# training_frames = validation_frames = testing_frames  = list(range(len(frames)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_generator = dg.DataGenerator(\n",
    "    config.input_image_channel,\n",
    "    config.patch_size,\n",
    "    validation_frames,\n",
    "    frames,\n",
    "    annotation_channels,\n",
    "    augmenter=None,\n",
    ").random_generator(config.BATCH_SIZE, normalize=config.normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = dg.DataGenerator(\n",
    "    config.input_image_channel,\n",
    "    config.patch_size,\n",
    "    testing_frames,\n",
    "    frames,\n",
    "    annotation_channels,\n",
    "    augmenter=None,\n",
    ").random_generator(config.BATCH_SIZE, normalize=config.normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1):\n",
    "    train_images, real_label = next(train_generator)\n",
    "    ann = real_label[:, :, :, 0]\n",
    "    wei = real_label[:, :, :, 1]\n",
    "    overlay = ann + wei\n",
    "    overlay = overlay[:, :, :, np.newaxis]\n",
    "    display_images(np.concatenate((train_images, real_label, overlay), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER = adaDelta\n",
    "LOSS = tversky\n",
    "\n",
    "# Only for the name of the model in the very end\n",
    "OPTIMIZER_NAME = \"AdaDelta\"\n",
    "LOSS_NAME = \"weightmap_tversky\"\n",
    "\n",
    "# Declare the path to the final model\n",
    "# If you want to retrain an exising model then change the cell where model is declared.\n",
    "# This path is for storing a model after training.\n",
    "\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M\")\n",
    "chf = config.input_image_channel + config.input_label_channel\n",
    "chs = reduce(lambda a, b: a + str(b), chf, \"\")\n",
    "\n",
    "model_path = config.model_dir/f\"trees_{timestr}_{OPTIMIZER_NAME}_{LOSS_NAME}_{chs}_{config.input_shape[0]}.h5\"\n",
    "\n",
    "# The weights without the model architecture can also be saved. Just saving the weights is more efficent.\n",
    "\n",
    "# weight_path=\"./saved_weights/UNet/{}/\".format(timestr)\n",
    "# if not os.path.exists(weight_path):\n",
    "#     os.makedirs(weight_path)\n",
    "# weight_path=weight_path + \"{}_weights.best.hdf5\".format('UNet_model')\n",
    "# print(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = reload(conf)\n",
    "config = conf.Configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model and compile it\n",
    "model = UNet([config.BATCH_SIZE, *config.input_shape], config.input_label_channel)\n",
    "model.compile(\n",
    "    optimizer=OPTIMIZER,\n",
    "    loss=LOSS,\n",
    "    metrics=[dice_coef, dice_loss, specificity, sensitivity, accuracy],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for the early stopping of training, LearningRateScheduler and model checkpointing\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    LearningRateScheduler,\n",
    "    EarlyStopping,\n",
    "    ReduceLROnPlateau,\n",
    "    TensorBoard,\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    model_path,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode=\"min\",\n",
    "    save_weights_only=False,\n",
    ")\n",
    "\n",
    "# reduceonplatea; It can be useful when using adam as optimizer\n",
    "# Reduce learning rate when a metric has stopped improving (after some patience,reduce by a factor of 0.33, new_lr = lr * factor).\n",
    "# cooldown: number of epochs to wait before resuming normal operation after lr has been reduced.\n",
    "reduceLROnPlat = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.33,\n",
    "    patience=4,\n",
    "    verbose=1,\n",
    "    mode=\"min\",\n",
    "    min_delta=0.0001,\n",
    "    cooldown=4,\n",
    "    min_lr=0.01,\n",
    ")\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=2, patience=20)\n",
    "\n",
    "log_dir = os.path.join(\n",
    "    \"./logs\",\n",
    "    \"UNet_{}_{}_{}_{}_{}\".format(\n",
    "        timestr, OPTIMIZER_NAME, LOSS_NAME, chs, config.input_shape[0]\n",
    "    ),\n",
    ")\n",
    "tensorboard = TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=0,\n",
    "    write_graph=True,\n",
    "    write_grads=False,\n",
    "    write_images=False,\n",
    "    embeddings_freq=0,\n",
    "    embeddings_layer_names=None,\n",
    "    embeddings_metadata=None,\n",
    "    embeddings_data=None,\n",
    "    update_freq=\"epoch\",\n",
    ")\n",
    "\n",
    "callbacks_list = [\n",
    "    checkpoint,\n",
    "    tensorboard,\n",
    "]  # reduceLROnPlat is not required with adaDelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = [\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=config.MAX_TRAIN_STEPS,\n",
    "        epochs=config.NB_EPOCHS,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=config.VALID_IMG_COUNT,\n",
    "        callbacks=callbacks_list,\n",
    "        workers=1,\n",
    "        # use_multiprocessing=True\n",
    "#                         use_multiprocessing=True # the generator is not very thread safe\n",
    "    )\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOeYCBzQRMr8FXNUC8za+ng",
   "collapsed_sections": [],
   "name": "step3-Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
